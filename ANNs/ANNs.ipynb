{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60EuJAhgQZo4"
   },
   "source": [
    "# 題目\n",
    "- 台股或美股，1. 預測收盤價(回歸)，2. 預測漲跌(分類)\n",
    "\n",
    "## 注意事項\n",
    "- 深度學習建議使用 Nvidia GPU , 激活 Pytorch CUDA 加速功能 , 在 Colab 推薦使用 Nvidia T4 之上的 GPU 加速訓練與推理速度\n",
    "\n",
    "## Colab 環境介紹\n",
    "- 整體為 VM (虛擬機)\n",
    "- OS : Ubuntu 22.04 LTS\n",
    "- CPU : 虛擬化 CPU 雙核約 2GHz\n",
    "- RAM : 約 12.7 GB , 頻率不一定\n",
    "- Disk : 約 112.6 GB , 讀寫速度不一定\n",
    "\n",
    "## NVIDIA T4 GPU (GPU直通)\n",
    "- NVIDIA CUDA 核心 : 2,560\n",
    "- VRAM : 16 GB GDDR6 , 可用約 15.0 GB , 頻寬320+ GB/s\n",
    "- FP32 : 8.1 TFLOPS\n",
    "- FP16/FP32 : 65 FP16 TFLOPS\n",
    "- INT8 : 130 INT8 TOPS\n",
    "- INT4 : 260 INT4 TOPS\n",
    "- PCle : Gen3 x16\n",
    "\n",
    "## 本地運算\n",
    "- RTX 4090 24G\n",
    "\n",
    "## 使用過去的程式碼 \n",
    "- [112-2 程式設計(Python)](https://github.com/TsukiSama9292/112-2_Python)\n",
    "- [113-1 大數據分析與智慧運算(期中報告)](https://github.com/TsukiSama9292/113-1_BigData)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本套件安裝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 台股資訊套件\n",
    "!pip install -U -qqq twstock\n",
    "!pip install -U -qqq lxml\n",
    "# 數據處理套件\n",
    "!pip install -U -qqq pandas numpy scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 獲取台灣股票資訊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已將 2330 在 2000 年至 2024 年 11 月的資訊儲存為 /2330_2000_to_202411.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from twstock import Stock\n",
    "import os\n",
    "# 求某年至 end_year end_month 月之前的資訊\n",
    "def get_stock_info_between_years_to_csv(stock_id, start_year=2000, end_year=2024, end_month=11):\n",
    "    # 使用 twstock 模組的 Stock 類來獲取股票資訊\n",
    "    stock = Stock(stock_id)\n",
    "\n",
    "    # 初始化一個空列表來存儲資訊\n",
    "    all_info = []\n",
    "    # 取從開始年到現今的紀錄\n",
    "    info = stock.fetch_from(start_year, 1)\n",
    "    all_info.extend(info)\n",
    "    # 找到 end_year 年 end_month 月之後的第一個索引\n",
    "    for i, data in enumerate(all_info):\n",
    "        if data.date.year == end_year and data.date.month >= end_month:\n",
    "            break\n",
    "    else:\n",
    "        i = len(all_info)  # 如果找不到 end_year 年 end_month 月之後的資訊，保留全部\n",
    "\n",
    "    # 刪除 end_year 年 end_month 月之後的紀錄\n",
    "    all_info = all_info[:i]\n",
    "    # 獲取當前工作目錄\n",
    "    current_directory = os.getcwd()  \n",
    "    # 將資訊寫入 CSV 檔案\n",
    "    global filename\n",
    "    filename = os.path.join(current_directory, f\"{stock_id}_{start_year}_to_{end_year}{end_month}.csv\")\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "\n",
    "        # 寫入 CSV 標題行\n",
    "        csvwriter.writerow(['Open', 'High', 'Low', 'Close', 'Change', 'Volume' , 'Date'])\n",
    "\n",
    "        # 寫入資訊行\n",
    "        for data in all_info:\n",
    "            csvwriter.writerow([data.open, data.high, data.low, data.close, data.change, data.capacity, data.date.strftime('%Y-%m-%d')])\n",
    "\n",
    "    print(f\"已將 {stock_id} 在 {start_year} 年至 {end_year} 年 {end_month} 月的資訊儲存為 {filename}\")\n",
    "\n",
    "# 使用範例，台機電股票\n",
    "get_stock_info_between_years_to_csv(stock_id='2330', start_year=2000, end_year=2024, end_month=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LItNHoKfqI_e"
   },
   "source": [
    "# 股票預測 - Pytorch 深度學習 - B1143015 林宣佑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import datetime\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current time: 2024_12_05_13_40_09\n",
      "scaled_features :  [0.00713322 0.00681317 0.00684262 0.00749173 0.21746397]\n",
      "Scaler 字典成功保存。\n",
      "train_features :  [0.00713322 0.00681317 0.00684262 0.00749173 0.21746397]\n",
      "*****CUDA Status*****\n",
      "CUDA Available: False\n",
      "****Data Status****\n",
      "Training data: 2900 records\n",
      "Testing data: 718 records\n"
     ]
    }
   ],
   "source": [
    "# Get the current time\n",
    "current_time = datetime.datetime.now()\n",
    "\n",
    "# Format the time\n",
    "formatted_time = current_time.strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "print(\"Current time:\", formatted_time)\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv(filename)\n",
    "\n",
    "# Select the necessary features and target variable\n",
    "features = data[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "target = data['Change']\n",
    "\n",
    "# Modify the target values\n",
    "def transform_target(value):\n",
    "    if value < 0:\n",
    "        return [1, 0, 0]  # Negative change\n",
    "    elif value == 0:\n",
    "        return [0, 1, 0]  # No change\n",
    "    else:\n",
    "        return [0, 0, 1]  # Positive change\n",
    "\n",
    "# Modify the target data into categorical data (originally [0,0,0])\n",
    "new_target = []\n",
    "for value in target:\n",
    "    new_target.append(transform_target(value))\n",
    "\n",
    "# Normalize the data using a dictionary of scalers\n",
    "scalers = {}\n",
    "scaled_features = []\n",
    "\n",
    "for feature in features.columns:\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_feature = scaler.fit_transform(features[[feature]])\n",
    "    scalers[feature] = scaler  # Save the scaler for each feature\n",
    "    scaled_features.append(scaled_feature)\n",
    "\n",
    "# Convert the list of scaled features back to a DataFrame\n",
    "scaled_features = np.hstack(scaled_features)\n",
    "print(\"scaled_features : \", scaled_features[0])\n",
    "\n",
    "# 保存 scaler 字典到檔案\n",
    "with open('scalers.pkl', 'wb') as f:\n",
    "    pickle.dump(scalers, f)\n",
    "\n",
    "print(\"Scaler 字典成功保存。\")\n",
    "\n",
    "# Split the data into training and testing sets, 80% training, 20% testing\n",
    "train_size = int(len(scaled_features) * 0.8)\n",
    "test_size = len(scaled_features) - train_size\n",
    "\n",
    "train_features = scaled_features[:train_size]\n",
    "train_target = new_target[:train_size]\n",
    "test_features = scaled_features[train_size:]\n",
    "test_target = new_target[train_size:]\n",
    "print(\"train_features : \",train_features[0])\n",
    "\n",
    "# Create input format for ANNs\n",
    "def create_sequences(features, target, time_step=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(features) - time_step):\n",
    "        X.append(features[i:(i + time_step), :])\n",
    "        y.append(target[i + time_step])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Time step for ANNs\n",
    "time_step = 10\n",
    "\n",
    "# Create training and testing data\n",
    "X_train, y_train = create_sequences(train_features, train_target, time_step)\n",
    "X_test, y_test = create_sequences(test_features, test_target, time_step)\n",
    "\n",
    "# 檢測設備是否支援 CUDA 加速，沒有 CUDA 則使用 CPU\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if USE_CUDA else 'cpu')\n",
    "print(\"*****CUDA Status*****\")\n",
    "print(f\"CUDA Available: {USE_CUDA}\")\n",
    "if USE_CUDA:\n",
    "    print(f\"CUDA Device: {torch.cuda.current_device()}\")\n",
    "    print(f\"CUDA Device Name: {torch.cuda.get_device_name(device)}\")\n",
    "\n",
    "# Convert training feature data to PyTorch tensor, specifying the data type as float32, then move it to the specified device (GPU or CPU)\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "\n",
    "# Convert training target data to PyTorch tensor, specifying the data type as float32, and reshape, then move it to the specified device (GPU or CPU)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "\n",
    "# Convert testing feature data to PyTorch tensor, specifying the data type as float32, then move it to the specified device (GPU or CPU)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "\n",
    "# Convert testing target data to PyTorch tensor, specifying the data type as float32, and reshape, then move it to the specified device (GPU or CPU)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "print(\"****Data Status****\")\n",
    "print(f\"Training data: {len(X_train_tensor)} records\")\n",
    "print(f\"Testing data: {len(X_test_tensor)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WcL4N-cgeEZy",
    "outputId": "6be5adf1-cefa-4726-ca4b-c06996b758f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*****Model Status*****\n",
      "Stock_ANNs(\n",
      "  (relu): Linear(in_features=5, out_features=180, bias=True)\n",
      "  (lstm): LSTM(180, 180, num_layers=2, batch_first=True, dropout=0.001)\n",
      "  (output): Linear(in_features=180, out_features=3, bias=True)\n",
      ")\n",
      "\n",
      "*****Training Status*****\n",
      "| epoch  10 | lr 0.0050000000 | 310.44 ms | loss 21.6342906 | train accuracy 48.38%\n",
      "| epoch  20 | lr 0.0048750000 | 298.56 ms | loss 21.7971879 | train accuracy 48.38%\n",
      "| epoch  30 | lr 0.0047531250 | 319.27 ms | loss 21.6059721 | train accuracy 48.38%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 71\u001b[0m\n\u001b[1;32m     69\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_y)  \u001b[38;5;66;03m# 計算模型的損失\u001b[39;00m\n\u001b[1;32m     70\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# 梯度反向傳播和參數更新\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     73\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class Stock_ANNs(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout):\n",
    "        super(Stock_ANNs, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.relu = nn.Linear(input_dim, hidden_dim)\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.relu(x)) # ReLU\n",
    "        # Initialize the hidden and cell states of the LSTM to zeros and move them to the specified device\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "        # Move the input data to the specified device and pass it to the LSTM layer, getting the output and new hidden and cell states\n",
    "        out, (hn, cn) = self.lstm(x.to(device), (h0.detach(), c0.detach()))\n",
    "        # Pass the LSTM output to the fully connected layer, using only the output of the last time step\n",
    "        out = self.output(out[:, -1, :])\n",
    "        # Move the final output to the specified device and return\n",
    "        return out.to(device)\n",
    "\n",
    "# Convert predicted values to labels\n",
    "def map_to_label(array):\n",
    "    labels = []\n",
    "    for row in array:\n",
    "        max_index = np.argmax(row)\n",
    "        if max_index == 0:\n",
    "            labels.append(-1)\n",
    "        elif max_index == 1:\n",
    "            labels.append(0)\n",
    "        elif max_index == 2:\n",
    "            labels.append(1)\n",
    "    return labels\n",
    "\n",
    "\n",
    "# 設定模型各層維度\n",
    "input_dim = X_train.shape[2] # 輸入\n",
    "hidden_dim = 180  # 隱藏 , 像神經一樣的概念\n",
    "num_layers = 2 # LSTM \n",
    "output_dim = 3   # 輸出\n",
    "dropout = 0.001 # 防止過擬合\n",
    "\n",
    "# 訓練參數設置\n",
    "batch_size = 128  # 批次大小\n",
    "epochs = 120     # 訓練次數\n",
    "lr = 5e-3        # 學習率\n",
    "criterion = nn.CrossEntropyLoss()  # 損失函數\n",
    "\n",
    "\n",
    "# 模型建立並移動到 device (CPU 或 GPU)\n",
    "model = Stock_ANNs(input_dim=input_dim, hidden_dim=hidden_dim, num_layers=num_layers, output_dim=output_dim, dropout=dropout).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)  # 優化器\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.975)  # 調節學習率工具\n",
    "print(\"\\n*****Model Status*****\")\n",
    "print(model)  # 顯示模型結構\n",
    "\n",
    "# 訓練模型\n",
    "print(\"\\n*****Training Status*****\")\n",
    "train_start = time.time()\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()  # 將模型設置為訓練模式\n",
    "    total_loss = 0  # 紀錄 LOSS\n",
    "    total_correct = 0  # 紀錄正確預測數量\n",
    "    total_samples = 0  # 紀錄樣本數量\n",
    "    start_time = time.time()  # 記錄當前epoch的開始時間\n",
    "    for i in range(0, len(X_train_tensor), batch_size):\n",
    "        batch_X = X_train_tensor[i:i + batch_size]  # 從訓練集中取出一個批次的特徵數據\n",
    "        batch_y = y_train_tensor[i:i + batch_size]  # 從訓練集中取出一個批次的目標數據\n",
    "        outputs = model(batch_X)  # 獲取模型的預測結果\n",
    "        loss = criterion(outputs, batch_y)  # 計算模型的損失\n",
    "        optimizer.zero_grad()  # 梯度反向傳播和參數更新\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 計算批次的準確率\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_correct += (predicted == batch_y.argmax(dim=1)).sum().item()\n",
    "        total_samples += batch_y.size(0)\n",
    "\n",
    "    # 計算整個epoch的準確率\n",
    "    train_accuracy = total_correct / total_samples\n",
    "\n",
    "    # 每十次輸出訓練狀態\n",
    "    # 第幾步驟 , 學習率, 訓練時間 , LOSS , 訓練資料準確率\n",
    "    if epoch % 10 == 0:\n",
    "        print('| epoch {:3d} | lr {:02.10f} | {:5.2f} ms | loss {:5.7f} | train accuracy {:3.2f}%'.format(\n",
    "            epoch, scheduler.get_last_lr()[0], (time.time() - start_time) * 1000, total_loss, train_accuracy * 100))\n",
    "    scheduler.step()  # 每個epoch結束後調整學習率(記數)\n",
    "\n",
    "train_times = time.time() - train_start  # 計算整個訓練過程的總執行時間\n",
    "print(f\"Training cost: {train_times:.2f} seconds\")  # 打印整個訓練過程的執行時間\n",
    "\n",
    "\n",
    "# 混淆矩陣和評估指標\n",
    "print(\"\\n*****Eval Status*****\")\n",
    "model.eval()\n",
    "test_outputs = model(X_test_tensor)\n",
    "\n",
    "predicted = test_outputs[10:].detach().cpu().numpy()\n",
    "actual = y_test_tensor[10:].cpu().numpy()\n",
    "\n",
    "# Convert predicted values and actual values to class labels\n",
    "predicted_labels = map_to_label(predicted)\n",
    "actual_labels = map_to_label(actual)\n",
    "\n",
    "# 打印混淆矩陣\n",
    "labels = list(range(output_dim))\n",
    "conf_matrix = confusion_matrix(actual_labels, predicted_labels, labels=labels)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# 打印 正確率 , 精確率 , 召回率 , F1-score\n",
    "print(f\"Accuracy: {accuracy_score(actual_labels, predicted_labels) * 100:.2f}%\")\n",
    "print(f\"Precision: {precision_score(actual_labels, predicted_labels, average='weighted', zero_division=0) * 100:.2f}%\")\n",
    "print(f\"Recall: {recall_score(actual_labels, predicted_labels, average='weighted', zero_division=0) * 100:.2f}%\")\n",
    "print(f\"F1 Score: {f1_score(actual_labels, predicted_labels, average='weighted', zero_division=0) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9iqduOlZOWEN",
    "outputId": "9638967d-d30d-4223-b33a-f87e0dc40a42"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from safetensors.torch import save_file\n",
    "import os\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    api_token = userdata.get('Colab_ALL')  # 在 Colab 中獲取 API_TOKEN\n",
    "except ImportError:\n",
    "    from dotenv import load_dotenv\n",
    "    import os\n",
    "    load_dotenv()  # 載入 .env 檔案\n",
    "    api_token = os.getenv('HF_API_TOKEN')  # 在其他環境中獲取 HF_API_TOKEN\n",
    "\n",
    "# 設定 Git 用戶名和電子郵件 , 改為自己 git 帳號\n",
    "!git config --global user.email \"a0985821880@gmail.com\"\n",
    "!git config --global user.name \"TsukiSama9292\"\n",
    "# 設定 hugging face 帳號和模型名稱\n",
    "USER_NAME = \"TsukiOwO\"\n",
    "MODEL_NAME = \"soft_decision_tree\"\n",
    "\n",
    "model_save_path = f\"{MODEL_NAME}.safetensors\"\n",
    "repo_name = f\"{USER_NAME}/{MODEL_NAME}\"\n",
    "api = HfApi()\n",
    "\n",
    "# 建構 HF 模型儲存庫\n",
    "try:\n",
    "    api.create_repo(repo_id=repo_name, token=api_token, private=False)\n",
    "    print(f\"儲存庫 '{repo_name}' 建立成功。\")\n",
    "except Exception as e:\n",
    "    print(f\"建構儲存庫操作時發生錯誤: {e}\")\n",
    "\n",
    "# 提取模型權重並保存為 safetensors 格式\n",
    "model_weights = {name: tensor for name, tensor in model.state_dict().items()}  # 提取每個張量\n",
    "save_file(model_weights, model_save_path)\n",
    "\n",
    "# 檢查檔案是否成功保存\n",
    "if os.path.exists(model_save_path):\n",
    "    print(f\"模型成功保存於 {model_save_path}。\")\n",
    "else:\n",
    "    print(f\"模型保存失敗於 {model_save_path}。\")\n",
    "\n",
    "\n",
    "# 上傳模型到 Hugging Face\n",
    "try:\n",
    "    # 使用 HfApi 上傳\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=model_save_path,\n",
    "        path_in_repo=model_save_path,\n",
    "        repo_id=repo_name,\n",
    "        token=api_token\n",
    "    )\n",
    "    print(f\"模型已上傳至 Hugging Face: {repo_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"儲存庫操作時發生錯誤: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXJ6lqOfmXRo"
   },
   "source": [
    "## 深度學習 - 測試模型 - B1143015 林宣佑, B1143028 詹朝成(架構修改)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PPjwl2mykD3c",
    "outputId": "b1d2ac36-9547-438c-f331-4af35f10b570"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from huggingface_hub import HfApi, HfFolder, Repository\n",
    "from safetensors.torch import save_file\n",
    "from google.colab import userdata\n",
    "# 檢測設備是否支援 CUDA 加速\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if USE_CUDA else 'cpu')\n",
    "print(\"*****CUDA Status*****\")\n",
    "print(f\"CUDA Available: {USE_CUDA}\")\n",
    "if USE_CUDA:\n",
    "    print(f\"CUDA Device: {torch.cuda.current_device()}\")\n",
    "    print(f\"CUDA Device Name: {torch.cuda.get_device_name(device)}\")\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 設定模型各層維度\n",
    "input_dim = 8    # 輸入\n",
    "hidden_dim = 150\n",
    "output_dim = 5   # 輸出\n",
    "\n",
    "# 轉換為 tensor 並移動到設備上\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device).squeeze()\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device).squeeze()\n",
    "\n",
    "# 訓練參數設置\n",
    "batch_size = 128  # 批次大小\n",
    "epochs = 100     # 訓練次數\n",
    "lr = 5e-3        # 學習率\n",
    "criterion = nn.CrossEntropyLoss()  # 損失函數\n",
    "\n",
    "# 模型建立並移動到 device (CPU 或 GPU)\n",
    "model = SimpleModel(input_dim=input_dim,hidden_dim=hidden_dim, output_dim=output_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)  # 優化器\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.95)  # 調節學習率工具\n",
    "print(\"\\n*****Model Status*****\")\n",
    "print(model)  # 顯示模型結構\n",
    "\n",
    "print(\"\\n*****Training Status*****\")\n",
    "# 訓練模型\n",
    "train_start = time.time()\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()  # 將模型設置為訓練模式\n",
    "    total_loss = 0  # 紀錄 LOSS\n",
    "    total_correct = 0  # 紀錄正確預測數量\n",
    "    total_samples = 0  # 紀錄樣本數量\n",
    "    start_time = time.time()  # 記錄當前epoch的開始時間\n",
    "    for i in range(0, len(X_train_tensor), batch_size):\n",
    "        batch_X = X_train_tensor[i:i + batch_size]  # 從訓練集中取出一個批次的特徵數據\n",
    "        batch_y = y_train_tensor[i:i + batch_size]  # 從訓練集中取出一個批次的目標數據\n",
    "        outputs = model(batch_X)  # 獲取模型的預測結果\n",
    "        loss = criterion(outputs, batch_y)  # 計算模型的損失\n",
    "        optimizer.zero_grad()  # 梯度反向傳播和參數更新\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 計算批次的準確率\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_correct += (predicted == batch_y).sum().item()\n",
    "        total_samples += batch_y.size(0)\n",
    "\n",
    "    # 計算整個epoch的準確率\n",
    "    train_accuracy = total_correct / total_samples\n",
    "\n",
    "    # 每十次輸出訓練狀態\n",
    "    # 第幾步驟 , 學習率, 訓練時間 , LOSS , 訓練資料準確率\n",
    "    if epoch % 10 == 0:\n",
    "        print('| epoch {:3d} | lr {:02.10f} | {:5.2f} ms | loss {:5.7f} | train accuracy {:3.2f}%'.format(\n",
    "            epoch, scheduler.get_last_lr()[0], (time.time() - start_time) * 1000, total_loss, train_accuracy * 100))\n",
    "\n",
    "    scheduler.step()  # 每個epoch結束後調整學習率(記數)\n",
    "\n",
    "train_times = time.time() - train_start  # 計算整個訓練過程的總執行時間\n",
    "print(f\"Training cost: {train_times:.2f} seconds\")  # 打印整個訓練過程的執行時間\n",
    "\n",
    "\n",
    "# 混淆矩陣和評估指標\n",
    "print(\"\\n*****Eval Status*****\")\n",
    "model.eval()\n",
    "test_outputs = model(X_test_tensor)\n",
    "\n",
    "# 使用 torch.argmax 獲取預測標籤\n",
    "predicted_labels = torch.argmax(test_outputs, dim=1).detach().cpu().numpy()\n",
    "actual_labels = y_test_tensor.cpu().numpy()\n",
    "\n",
    "# 打印混淆矩陣\n",
    "labels = list(range(output_dim))\n",
    "conf_matrix = confusion_matrix(actual_labels, predicted_labels, labels=labels)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# 打印 正確率 , 精確率 , 召回率\n",
    "print(f\"Accuracy: {accuracy_score(actual_labels, predicted_labels) * 100:.2f}%\")\n",
    "print(f\"Precision: {precision_score(actual_labels, predicted_labels, average='weighted', zero_division=0) * 100:.2f}%\")\n",
    "print(f\"Recall: {recall_score(actual_labels, predicted_labels, average='weighted', zero_division=0) * 100:.2f}%\")\n",
    "print(f\"F1 Score: {f1_score(actual_labels, predicted_labels, average='weighted', zero_division=0) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fL9O-LbUEvhC"
   },
   "source": [
    "## 深度學習 - 自注意力模型 - B1143015 林宣佑, B1143028 詹朝成(架構修改)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gpdWUURpEu4U",
    "outputId": "7b47382f-cc0a-4239-a32f-dc305029f35e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from huggingface_hub import HfApi, HfFolder, Repository\n",
    "from safetensors.torch import save_file\n",
    "from google.colab import userdata\n",
    "# 檢測設備是否支援 CUDA 加速\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if USE_CUDA else 'cpu')\n",
    "print(\"*****CUDA Status*****\")\n",
    "print(f\"CUDA Available: {USE_CUDA}\")\n",
    "if USE_CUDA:\n",
    "    print(f\"CUDA Device: {torch.cuda.current_device()}\")\n",
    "    print(f\"CUDA Device Name: {torch.cuda.get_device_name(device)}\")\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query = nn.Linear(input_dim, input_dim)\n",
    "        self.key = nn.Linear(input_dim, input_dim)\n",
    "        self.value = nn.Linear(input_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        attention_scores = torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(x.size(-1))\n",
    "        attention_probs = torch.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        return torch.matmul(attention_probs, v)\n",
    "\n",
    "# 定義結合模型：自注意力 + ReLU\n",
    "class AttentionWithReLU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(AttentionWithReLU, self).__init__()\n",
    "        self.relu1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.attention = SelfAttention(hidden_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.relu1(x))\n",
    "        x = self.attention(x)  # 自注意力層處理輸入\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "# 設定模型各層維度\n",
    "input_dim = 8    # 輸入\n",
    "hidden_dim = 100  # 隱藏 , 像神經一樣的概念\n",
    "output_dim = 5   # 輸出\n",
    "\n",
    "# 轉換為 tensor 並移動到設備上\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device).squeeze()\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device).squeeze()\n",
    "\n",
    "# 訓練參數設置\n",
    "batch_size = 128  # 批次大小\n",
    "epochs = 100     # 訓練次數\n",
    "lr = 5e-3        # 學習率\n",
    "criterion = nn.CrossEntropyLoss()  # 損失函數\n",
    "\n",
    "# 模型建立並移動到 device (CPU 或 GPU)\n",
    "model = AttentionWithReLU(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)  # 優化器\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.95)  # 調節學習率工具\n",
    "print(\"\\n*****Model Status*****\")\n",
    "print(model)  # 顯示模型結構\n",
    "\n",
    "# 訓練模型\n",
    "train_start = time.time()\n",
    "print(\"\\n*****Training Status*****\")\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()  # 將模型設置為訓練模式\n",
    "    total_loss = 0  # 紀錄 LOSS\n",
    "    total_correct = 0  # 紀錄正確預測數量\n",
    "    total_samples = 0  # 紀錄樣本數量\n",
    "    start_time = time.time()  # 記錄當前epoch的開始時間\n",
    "    for i in range(0, len(X_train_tensor), batch_size):\n",
    "        batch_X = X_train_tensor[i:i + batch_size]  # 從訓練集中取出一個批次的特徵數據\n",
    "        batch_y = y_train_tensor[i:i + batch_size]  # 從訓練集中取出一個批次的目標數據\n",
    "        outputs = model(batch_X)  # 獲取模型的預測結果\n",
    "        loss = criterion(outputs, batch_y)  # 計算模型的損失\n",
    "        optimizer.zero_grad()  # 梯度反向傳播和參數更新\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 計算批次的準確率\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_correct += (predicted == batch_y).sum().item()\n",
    "        total_samples += batch_y.size(0)\n",
    "\n",
    "    # 計算整個epoch的準確率\n",
    "    train_accuracy = total_correct / total_samples\n",
    "\n",
    "    # 每十次輸出訓練狀態\n",
    "    # 第幾步驟 , 學習率, 訓練時間 , LOSS , 訓練資料準確率\n",
    "    if epoch % 10 == 0:\n",
    "        print('| epoch {:3d} | lr {:02.10f} | {:5.2f} ms | loss {:5.7f} | train accuracy {:3.2f}%'.format(\n",
    "            epoch, scheduler.get_last_lr()[0], (time.time() - start_time) * 1000, total_loss, train_accuracy * 100))\n",
    "\n",
    "    scheduler.step()  # 每個epoch結束後調整學習率(記數)\n",
    "\n",
    "train_times = time.time() - train_start  # 計算整個訓練過程的總執行時間\n",
    "print(f\"Training cost: {train_times:.2f} seconds\")  # 打印整個訓練過程的執行時間\n",
    "\n",
    "\n",
    "# 混淆矩陣和評估指標\n",
    "print(\"\\n*****Eval Status*****\")\n",
    "model.eval()\n",
    "test_outputs = model(X_test_tensor)\n",
    "\n",
    "# 使用 torch.argmax 獲取預測標籤\n",
    "predicted_labels = torch.argmax(test_outputs, dim=1).detach().cpu().numpy()\n",
    "actual_labels = y_test_tensor.cpu().numpy()\n",
    "\n",
    "# 打印混淆矩陣\n",
    "labels = list(range(output_dim))\n",
    "conf_matrix = confusion_matrix(actual_labels, predicted_labels, labels=labels)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# 打印 正確率 , 精確率 , 召回率\n",
    "print(f\"Accuracy: {accuracy_score(actual_labels, predicted_labels) * 100:.2f}%\")\n",
    "print(f\"Precision: {precision_score(actual_labels, predicted_labels, average='weighted', zero_division=0) * 100:.2f}%\")\n",
    "print(f\"Recall: {recall_score(actual_labels, predicted_labels, average='weighted', zero_division=0) * 100:.2f}%\")\n",
    "print(f\"F1 Score: {f1_score(actual_labels, predicted_labels, average='weighted', zero_division=0) * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "w79Is_as2S6F",
    "LItNHoKfqI_e",
    "wXJ6lqOfmXRo"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
